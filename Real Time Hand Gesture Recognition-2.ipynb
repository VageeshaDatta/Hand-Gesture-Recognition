{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046489a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "WARNING:tensorflow:From /opt/anaconda3/lib/python3.8/site-packages/tflearn/initializations.py:110: calling UniformUnitScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/util/deprecation.py:538: UniformUnitScaling.__init__ (from tensorflow.python.ops.init_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.initializers.variance_scaling instead with distribution=uniform to get equivalent behavior.\n",
      "WARNING:tensorflow:From /opt/anaconda3/lib/python3.8/site-packages/tflearn/initializations.py:164: calling TruncatedNormal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:201: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-20 09:18:19.710096: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-12-20 09:18:19.710360: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-12-20 09:18:20.872263: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:196] None of the MLIR optimization passes are enabled (registered 0 passes)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /Users/vageeshadatta/Desktop/NewTrainedModel/GestureRecogModel.tfl\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tflearn\n",
    "from tflearn.layers.conv import conv_2d,max_pool_2d\n",
    "from tflearn.layers.core import input_data,dropout,fully_connected\n",
    "from tflearn.layers.estimator import regression\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import imutils\n",
    "\n",
    "# global variables\n",
    "bg = None\n",
    "\n",
    "def resizeImage(imageName):\n",
    "    basewidth = 100\n",
    "    img = Image.open(imageName)\n",
    "    wpercent = (basewidth/float(img.size[0]))\n",
    "    hsize = int((float(img.size[1])*float(wpercent)))\n",
    "    img = img.resize((basewidth,hsize), Image.ANTIALIAS)\n",
    "    img.save(imageName)\n",
    "\n",
    "def run_avg(image, aWeight):\n",
    "    global bg\n",
    "    # initialize the background\n",
    "    if bg is None:\n",
    "        bg = image.copy().astype(\"float\")\n",
    "        return\n",
    "\n",
    "    # compute weighted average, accumulate it and update the background\n",
    "    cv2.accumulateWeighted(image, bg, aWeight)\n",
    "\n",
    "def segment(image, threshold=25):\n",
    "    global bg\n",
    "    # find the absolute difference between background and current frame\n",
    "    diff = cv2.absdiff(bg.astype(\"uint8\"), image)\n",
    "\n",
    "    # threshold the diff image so that we get the foreground\n",
    "    thresholded = cv2.threshold(diff,\n",
    "                                threshold,\n",
    "                                255,\n",
    "                                cv2.THRESH_BINARY)[1]\n",
    "\n",
    "    # get the contours in the thresholded image\n",
    "    (cnts, _) = cv2.findContours(thresholded.copy(),\n",
    "                                    cv2.RETR_EXTERNAL,\n",
    "                                    cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # return None, if no contours detected\n",
    "    if len(cnts) == 0:\n",
    "        return\n",
    "    else:\n",
    "        # based on contour area, get the maximum contour which is the hand\n",
    "        segmented = max(cnts, key=cv2.contourArea)\n",
    "        return (thresholded, segmented)\n",
    "\n",
    "def main():\n",
    "    # initialize weight for running average\n",
    "    aWeight = 0.5\n",
    "\n",
    "    # get the reference to the webcam\n",
    "    camera = cv2.VideoCapture(0)\n",
    "\n",
    "    # region of interest (ROI) coordinates\n",
    "    top, right, bottom, left = 10, 350, 225, 590\n",
    "\n",
    "    # initialize num of frames\n",
    "    num_frames = 0\n",
    "    start_recording = False\n",
    "\n",
    "    # keep looping, until interrupted\n",
    "    while(True):\n",
    "        # get the current frame\n",
    "        (grabbed, frame) = camera.read()\n",
    "\n",
    "        # resize the frame\n",
    "        frame = imutils.resize(frame, width = 700)\n",
    "\n",
    "        # flip the frame so that it is not the mirror view\n",
    "        frame = cv2.flip(frame, 1)\n",
    "\n",
    "        # clone the frame\n",
    "        clone = frame.copy()\n",
    "\n",
    "        # get the height and width of the frame\n",
    "        (height, width) = frame.shape[:2]\n",
    "\n",
    "        # get the ROI\n",
    "        roi = frame[top:bottom, right:left]\n",
    "\n",
    "        # convert the roi to grayscale and blur it\n",
    "        gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)\n",
    "        gray = cv2.GaussianBlur(gray, (7, 7), 0)\n",
    "\n",
    "        # to get the background, keep looking till a threshold is reached\n",
    "        # so that our running average model gets calibrated\n",
    "        if num_frames < 30:\n",
    "            run_avg(gray, aWeight)\n",
    "        else:\n",
    "            # segment the hand region\n",
    "            hand = segment(gray)\n",
    "\n",
    "            # check whether hand region is segmented\n",
    "            if hand is not None:\n",
    "                # if yes, unpack the thresholded image and\n",
    "                # segmented region\n",
    "                (thresholded, segmented) = hand\n",
    "\n",
    "                # draw the segmented region and display the frame\n",
    "                cv2.drawContours(clone, [segmented + (right, top)], -1, (0, 0, 255))\n",
    "                if start_recording:\n",
    "                    cv2.imwrite('Temp.png', thresholded)\n",
    "                    resizeImage('Temp.png')\n",
    "                    predictedClass, confidence = getPredictedClass()\n",
    "                    showStatistics(predictedClass, confidence)\n",
    "                cv2.imshow(\"Thesholded\", thresholded)\n",
    "\n",
    "        # draw the segmented hand\n",
    "        cv2.rectangle(clone, (left, top), (right, bottom), (0,255,0), 2)\n",
    "\n",
    "        # increment the number of frames\n",
    "        num_frames += 1\n",
    "\n",
    "        # display the frame with segmented hand\n",
    "        cv2.imshow(\"Video Feed\", clone)\n",
    "\n",
    "        # observe the keypress by the user\n",
    "        keypress = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "        # if the user pressed \"q\", then stop looping\n",
    "        if keypress == ord(\"q\"):\n",
    "            break\n",
    "        \n",
    "        if keypress == ord(\"s\"):\n",
    "            start_recording = True\n",
    "\n",
    "def getPredictedClass():\n",
    "    # Predict\n",
    "    image = cv2.imread('Temp.png')\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    prediction = model.predict([gray_image.reshape(89, 100, 1)])\n",
    "    return np.argmax(prediction), (np.amax(prediction) / (prediction[0][0] + prediction[0][1] + prediction[0][2]))\n",
    "\n",
    "def showStatistics(predictedClass, confidence):\n",
    "\n",
    "    textImage = np.zeros((300,512,3), np.uint8)\n",
    "    className = \"\"\n",
    "\n",
    "    \n",
    "    if predictedClass == 0:\n",
    "        className = \"Swing\"\n",
    "    elif predictedClass == 1:\n",
    "        className = \"Palm\"\n",
    "    elif predictedClass == 2:\n",
    "        className = \"Fist\"\n",
    "\n",
    "    cv2.putText(textImage,\"Pedicted Class : \" + className, \n",
    "    (30, 30), \n",
    "    cv2.FONT_HERSHEY_SIMPLEX, \n",
    "    1,\n",
    "    (255, 255, 255),\n",
    "    2)\n",
    "\n",
    "    cv2.putText(textImage,\"Confidence : \" + str(confidence * 100) + '%', \n",
    "    (30, 100), \n",
    "    cv2.FONT_HERSHEY_SIMPLEX, \n",
    "    1,\n",
    "    (255, 255, 255),\n",
    "    2)\n",
    "    cv2.imshow(\"Predictions\", textImage)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Model defined\n",
    "tf.compat.v1.reset_default_graph()\n",
    "convnet=input_data(shape=[None,89,100,1],name='input')\n",
    "convnet=conv_2d(convnet,32,2,activation='relu')\n",
    "convnet=max_pool_2d(convnet,2)\n",
    "convnet=conv_2d(convnet,64,2,activation='relu')\n",
    "convnet=max_pool_2d(convnet,2)\n",
    "\n",
    "convnet=conv_2d(convnet,128,2,activation='relu')\n",
    "convnet=max_pool_2d(convnet,2)\n",
    "\n",
    "convnet=conv_2d(convnet,256,2,activation='relu')\n",
    "convnet=max_pool_2d(convnet,2)\n",
    "\n",
    "convnet=conv_2d(convnet,256,2,activation='relu')\n",
    "convnet=max_pool_2d(convnet,2)\n",
    "\n",
    "convnet=conv_2d(convnet,128,2,activation='relu')\n",
    "convnet=max_pool_2d(convnet,2)\n",
    "\n",
    "convnet=conv_2d(convnet,64,2,activation='relu')\n",
    "convnet=max_pool_2d(convnet,2)\n",
    "\n",
    "convnet=fully_connected(convnet,1000,activation='relu')\n",
    "convnet=dropout(convnet,0.75)\n",
    "\n",
    "convnet=fully_connected(convnet,3,activation='softmax')\n",
    "\n",
    "convnet=regression(convnet,optimizer='adam',learning_rate=0.001,loss='categorical_crossentropy',name='regression')\n",
    "\n",
    "model=tflearn.DNN(convnet,tensorboard_verbose=0)\n",
    "\n",
    "# Load Saved Model\n",
    "model.load(\"/Users/vageeshadatta/Desktop/NewTrainedModel/GestureRecogModel.tfl\")\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff9c0a49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tflearn\n",
      "  Downloading tflearn-0.5.0.tar.gz (107 kB)\n",
      "\u001b[K     |████████████████████████████████| 107 kB 3.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy in /opt/anaconda3/lib/python3.8/site-packages (from tflearn) (1.21.2)\n",
      "Requirement already satisfied: six in /opt/anaconda3/lib/python3.8/site-packages (from tflearn) (1.16.0)\n",
      "Requirement already satisfied: Pillow in /opt/anaconda3/lib/python3.8/site-packages (from tflearn) (8.4.0)\n",
      "Building wheels for collected packages: tflearn\n",
      "  Building wheel for tflearn (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for tflearn: filename=tflearn-0.5.0-py3-none-any.whl size=127299 sha256=fe73860357bb9b70bba9d0e7639e9b7ee7c5c62dc773deb132be2058ab74fd3f\n",
      "  Stored in directory: /Users/vageeshadatta/Library/Caches/pip/wheels/65/9b/15/cb1e6b279c14ed897530d15cfd7da8e3df8a947e593f5cfe59\n",
      "Successfully built tflearn\n",
      "Installing collected packages: tflearn\n",
      "Successfully installed tflearn-0.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tflearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f21be220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting imutils\n",
      "  Downloading imutils-0.5.4.tar.gz (17 kB)\n",
      "Building wheels for collected packages: imutils\n",
      "  Building wheel for imutils (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for imutils: filename=imutils-0.5.4-py3-none-any.whl size=25860 sha256=a503a594a7aaae8353771420f40ed64557bbce1d59ff8fc78aae5e0aca295f44\n",
      "  Stored in directory: /Users/vageeshadatta/Library/Caches/pip/wheels/59/1b/52/0dea905f8278d5514dc4d0be5e251967f8681670cadd3dca89\n",
      "Successfully built imutils\n",
      "Installing collected packages: imutils\n",
      "Successfully installed imutils-0.5.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install imutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6cb2659",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
